<!DOCTYPE html><html lang="en"><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@xz/fonts@1.0.0/serve/inter.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@vladocar/basic.css@1.0.3/css/basic.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/sakura.css@1.4.0/css/sakura.min.css">
    <link rel="webmention" href="https://webmention.io/weiji14.github.io/webmention">
    <link rel="pingback" href="https://webmention.io/weiji14.github.io/xmlrpc">
    <link rel="alternate" href="https://weiji14.github.io/.well-known/webfinger" type="application/jrd+json">
    <link rel="alternate" href="https://weiji14.github.io/weblog" type="application/activity+json">
    <title>The Pangeo Machine Learning Ecosystem in 2023</title>
  </head>

  <body>
    <header>
      <nav>
        <a href="/">Home</a> /
        <a href="/blog">Blog</a> /
        <a href="/cv">CV</a> /
        <b>Experience:</b>
        <a href="/community">Community</a> /
        <a href="/academic">Academic</a> /
        <a href="/professional">Professional</a>
      </nav>
    </header>
    <main>
      <h1>The Pangeo Machine Learning Ecosystem in 2023</h1>
      <h3>TLDR</h3>
<p>Open source tools developed by the Pangeo ML community are enabling the shift to cloud-native geospatial Machine Learning.
Join the <a href="https://pangeo.io/meeting-notes.html#working-group-meetings">Pangeo ML community</a> working in towards scalable <a href="./xarray-kvikio">GPU-native</a> workflows! ðŸš€</p>
<h3>Overview</h3>
<h4>Building next-generation Machine Learning (ML) tools</h4>
<p>At FOSS4G SotM Oceania 2023, we presented on "The ecosystem of geospatial machine learning tools in the Pangeo World" (see the recording <a href="https://www.youtube.com/watch?v=X2LBuUfSo5Q">here</a>).
One of the driving forces of the Pangeo community is to build better tools that will enable scientific workflows on petabyte-scale datasets, such as Climate/Weather projections that will impact the planet over the coming decades.</p>
<p>To do that, we need to be fast.</p>
<p>These next-generation tools need to be scalable, efficient, and modular.
So we are designing them with three aspects in mind:</p>
<ul>
<li>Work with cloud-native data using <strong>GPU-native</strong> compute</li>
<li>Be able to <strong>stream</strong> subsets of data on-the-fly</li>
<li>Go from single sensor to <strong>multi-modal</strong> models</li>
</ul>
<p>Neither of these core technologies are particularly new.
NVIDIA has been leading the development of GPU-native <a href="https://rapids.ai">RAPIDS AI</a> libraries since 2018.
Streaming has been around since the 2010s if not earlier, and is practically the most common way to consume music and video content nowadays.
Since then, we have also seen the rise in <a href="https://doi.org/10.48550/arXiv.2309.10020">multi-modal Foundation Models</a> that are able to take in visual (image) and language (text and sound) cues.</p>
<p>Let's now take a step back, and picture what we're working with.</p>
<h3>Layers of the Pangeo Machine Learning stack</h3>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABkAAAAQCAYAAADj5tSrAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEAElEQVR4nE2UaY/byBGG9f9/zQbYDwYWSRx7F/bYnssz1uiWePd9kJRInU9A2gn2w4tGFarr6e7qqsmnHzN+v/vK73ff+Lbcsi4M69yQSYeygRAjdV3TNA1t2476nz2sg0IIo/7uH+wh9nQ6Mfk4f+Mfj1/47f4rnxZr3hLJ206yLQyVcljn8d6PsPArmXNu9I1+77HWYoz5af8tZgAej0cmj9mO96sp/15M+bra8LLKeZonvK0zdllJWVYIIVBSjomGzcOqlaKqKsqyJC8KiqIY46SU/1eM8SfE9S1qH1BtJDR7vG/RKmD0cNKI9R7lLJXVuBjYHw50XUesa5JSskoLXldrlrs1m3RDURVIJcfbDHHX65XJ5XTjfLxyPNw41Bei7bCixoqINzXGOkqjyI3E1pHueOR0OqNc4H6e8OnHmj+fX3hZTHl4eeL19Y3VcotWhv7Yc7vdmBzbG42+4dMbdnvDJRd8caJWPdG2aGPJZEWmJbaJ9OcTp/MF5RvuFzmf3wbQjB/LOR8+3vHPd3fcfXwlTyTn8+knpG9uhOyGnN4on2+o2QC80tozbX3Au4AyGm0NPgT2+z1N0yJtYJlrNpUl1w5tHY/fFrx/98zdhxlV7rjdrr8g+wuu6BHLjnJ6xG6vqKxDioDSPwsshWC1WvPyOuX+6Ttv8yV5WWFDQ2x7QttjY8dsLvnyecv3xxQl41iPEdJ2BwpRkaQlRWKo9QlZBPJ1SrnZYIoMqzXzxYq7bw/868Nf3D88s9sVxNDR7i+Y5oiqOzJVk+aRMm/w6kjfXjl1NybNvqbQKblKMF7S9z3Oespd8guSU0nD83zLf7688MfHrzw8zdluJFZ3ONezU56l1LwJyUYONYwI2VKHI6fuOkAadnnGJk0pckHtW0zlkOscs8sJQiJtzUZ4ZqlhlWlKETHqgBZ7qiIy2xa8blOmWUYqDblwLHeapHBof2DSdT1CBqrSo0SgjXtqv8fpiDMBazyFdGwrx6pwpMIjVBzfvMo1eSJYbBIWScZalIjgya3mabvh+y5hayyTvj+hTYOSNVpGateMAFlZlDBIaZDaIo1DjL/IY90wNjxKaYpCkOdDp1ti3dL3R5p2z7IomeUFuwFy1JFGBGJmCbkjnaVslhnzxY5KKkqpyLUmlYpkGC/GUgk1Jk+SjCzLEUKOzWmbHrM/I2LHSng2ylOGlslZR86u4SwD+8qzeFjy8rTg++sS4zxh6ImmIfWBjTaoWKNMQBtPVSmkUmitx75RsUM0Z1R7QbYX3OFKexzGiqk5u5aLiOxzx49PUx6+THl8mhNiTdt1yLZlZRyvlSCznkI5pHKUpUSNEDVCRDhQ1GeK+kIez6j9hdhf+S8jM9OZwQxNGgAAAABJRU5ErkJggg==" alt="Pangeo Machine Learning Ecosystem in 2023. Bottom row shows cloud-optimized file formats. Middle row shows Array libraries. Top row shows the Pangeo ML libraries and educational resources." data-src="https://github.com/weiji14/foss4g2023oceania/releases/download/v0.9.0/pangeo_ml_ecosystem.png" class="lazyload" width="2795" height="1745"></p>
<p>There are three main layers to a Machine Learning data pipeline.
It starts with data storage file formats at the bottom row, an in-memory array representation in the middle, and high-level libraries and documentation resources that users or developers interact with at the top.</p>
<p>The key to connecting all of these layers are open standards.</p>
<h4>Cloud-native geospatial file formats</h4>
<p>For the file formats, we favour <a href="https://www.ogc.org/ogc-topics/cloud-native-geospatial">cloud-native geospatial</a> because it allows us to efficiently access subsets of data without reading the entire file.
Generally speaking, you would store rasters as <a href="https://zarr.dev">Zarr</a> or <a href="https://www.cogeo.org">Cloud-Optimized GeoTIFFs</a>, and vectors (points/lines/polygons) in <a href="https://flatgeobuf.org">FlatGeobuf</a> or <a href="https://geoparquet.org">(Geo)Parquet</a>.
Ideally though, these files would be indexed using a <a href="https://stacspec.org">SpatioTemporal Asset Catalog (STAC)</a> which makes it easier to discover datasets using standardized queries.
This can be a whole topic in itself, so check out this <a href="https://guide.cloudnativegeo.org">guide</a> <a href="https://cloudnativegeo.org/blog/2023/10/introducing-the-cloud-optimized-geospatial-formats-guide">published in October 2023</a> for more details!</p>
<h4>In memory array representations</h4>
<p>In the Python world, <a href="https://numpy.org">NumPy</a> arrays have been the core way of representing arrays in-memory, but there are many others too, along with an ongoing movement to standardize the array/dataframe API at <a href="https://data-apis.org">https://data-apis.org</a>.
Geospatial folks would most likely be familiar with vector libraries like <a href="https://geopandas.org">GeoPandas</a> GeoDataFrames (built on top of <a href="https://pandas.pydata.org">pandas</a>); or raster libraries like <a href="https://corteva.github.io/rioxarray">rioxarray</a> and <a href="https://stackstac.readthedocs.io">stackSTAC</a> that reads into <a href="https://xarray.dev">xarray</a> data structures.</p>
<p>NumPy arrays are CPU-based, but there are also libraries like <a href="https://cupy.dev">CuPy</a> which can do GPU-accelerated computations.
Instead of GeoPandas, you could use libraries like <a href="https://docs.rapids.ai/api/cuspatial">cuSpatial</a> (built on top of <a href="https://docs.rapids.ai/api/cudf">cuDF</a> and part of <a href="https://rapids.ai">RAPIDS AI</a>) to run GPU-accelerated algorithms.
Deep Learning libraries like <a href="https://pytorch.org/docs">PyTorch</a>, <a href="https://www.tensorflow.org">TensorFlow</a> or <a href="https://jax.readthedocs.io">JaX</a> tend to be GPU-based as well, but there are also libraries like <a href="https://datashader.org">Datashader</a> (for visualization) and <a href="https://xarray.dev">Xarray</a> that are designed to be CPU/GPU agnostic and can hold either.</p>
<h4>High-level Pangeo ML libraries</h4>
<p>Finally, to make life simpler, we have high-level convenience libraries wrapping the low-level stuff.
These are designed to have a nicer user interface to connect the underlying file formats and in-memory array representations.
The <a href="https://pangeo.io/meeting-notes.html#working-group-meetings">Pangeo Machine Learning Working Group</a> mostly works on Climate/Weather datasets, so we'll focus on multi-dimensional arrays for now.</p>
<p>Stepping into the GPU-native world, <a href="https://cupy-xarray.readthedocs.io">cupy-xarray</a> allows users to use GPU-backed CuPy arrays in n-dimensional Xarray data structures (see our previous <a href="./cupy-tutorial">blog post</a> on this).
An exciting development on this front is the experimental <a href="https://github.com/rapidsai/kvikio">kvikIO</a> engine that enables low-latency reading data from Zarr stores into GPU memory using NVIDIA GPUDirect Storage technology (see this <a href="./xarray-kvikio">blog post</a>).
<a href="https://github.com/zarr-developers/zarr-benchmark/discussions/14">Preliminary benchmarks</a> suggest that the GPU-based <code>kvikIO</code> engine can take about 25% less time for data reads compared to the regular CPU-based <code>zarr</code> engine!</p>
<p>Once you have tensors loaded (lazily) into an Xarray data structure, <a href="https://xbatcher.readthedocs.io">xbatcher</a> enables efficient iteration over batches of data in a streaming fashion.
This library makes it easier to train machine learning models on big datacubes such as time-series datasets or multi-variate ocean/climate model outputs, as users can do on-the-fly slicing using named variables (more readable than numbered indexes).
There is also an experimental <a href="https://github.com/xarray-contrib/xbatcher/pull/167">cache mechanism</a> we'd like more people to try and provide feedback on!</p>
<p>To connect all of the pieces, <a href="https://zen3geo.readthedocs.io">zen3geo</a> implements Composable DataPipes for geospatial.
It acts as the glue to chain together different building blocks, such as readers for Vector/Raster file formats, converters between different in-memory array representations, and even custom pre-processing functions.
The composable design pattern makes it well suited for building complex machine learning data pipelines for multi-modal models that can take in different inputs (e.g. Images, Point Clouds, Trajectory, Text/Sound, etc).
Going forward, there are plans to <a href="https://github.com/weiji14/zen3geo/discussions/117">refactor the backend to be asynchronous-first</a> to overcome I/O bottlenecks.</p>
<h3>Summary</h3>
<p>We've presented a snapshot of the Pangeo Machine Learning ecosystem in 2023.
The basis of any machine learning project is the data, and we touched on how cloud-native geospatial file formats and in-memory array representations built on open standards act as the foundation for our work.
Lastly, we highlight some of the high-level Pangeo ML libraries enabling user friendly access to GPU-native compute, streaming data batches, and composable geospatial data pipelines.</p>
<h3>Where to learn more</h3>
<ul>
<li>
<p>Educational resources:</p>
<ul>
<li><a href="https://cookbooks.projectpythia.org">Project Pythia Cookbooks</a></li>
<li><a href="https://geo-smart.github.io/mlgeo-book">GeoSMART Machine Learning Curriculum</a></li>
<li><a href="https://guidebook.hackweek.io">University of Washington Hackweeks as a Service</a></li>
</ul>
</li>
<li>
<p>Pangeo ML Working Group:</p>
<ul>
<li><a href="https://pangeo.io/meeting-notes.html#working-group-meetings">Monthly meetings</a></li>
<li><a href="https://discourse.pangeo.io/tag/machine-learning">Discourse Forum</a></li>
</ul>
</li>
</ul>
<h3>Acknowledgments</h3>
<p>The work above is the cumulative effort of folks from the Pangeo, Xarray and RAPIDS AI community, plus more!
In particular, we'd like to acknowledge the work of <a href="https://github.com/dcherian">Deepak Cherian</a> at <a href="https://earthmover.io">Earthmover</a> and <a href="https://github.com/negin513">Negin Sobhani</a> at <a href="https://ncar.ucar.edu">NCAR</a> for their work on cupy-xarray/kvikIO,
<a href="https://github.com/maxrjones">Max Jones</a> at <a href="https://carbonplan.org">Carbonplan</a> for recent developments on the xbatcher package,
and <a href="https://github.com/weiji14">Wei Ji Leong</a> at <a href="https://developmentseed.org">Development Seed</a> for the development of zen3geo.</p>
<h3>Appendix I: Further Reading</h3>
<ul>
<li><a href="https://voltrondata.com/codex">The Composable Codex</a></li>
<li><a href="https://discourse.pangeo.io/t/monday-november-07-2022-machine-learning-working-group-presentation-zen3geo-guiding-earth-observation-data-on-its-path-to-enlightenment-by-wei-ji-leong/2883">zen3geo 2022 Pangeo ML Working Group presentation</a> (<a href="https://www.youtube.com/watch?v=8uhOtQUTuDg">recording</a>)</li>
<li><a href="https://doi.org/10.6084/m9.figshare.22264072.v1">Xbatcher 2023 AMS presentation</a> (<a href="https://ams.confex.com/recording/ams/103ANNUAL/mp4/CGNTFL54WCL/67cfb841cba94216ff99f1eb15286ba2/session63444_5.mp4">recording</a> (starts at 45:30))</li>
<li><a href="https://doi.org/10.5281/zenodo.8247471">CuPy-Xarray tutorial at SciPy 2023</a> (<a href="https://negin513.github.io/cupy-xarray-tutorials/README.html">jupyter-book</a>)</li>
<li><a href="https://github.com/weiji14/foss4g2023oceania">Pangeo ML Ecosystem presentation at FOSS4G SotM Oceania 2023</a> (<a href="https://www.youtube.com/watch?v=X2LBuUfSo5Q">recording</a>)</li>
</ul>
<hr>
<p>Note: A version of this blog post will be published at <a href="https://xarray.dev/blog">https://xarray.dev/blog</a> once <a href="https://github.com/xarray-contrib/xarray.dev/pull/625">https://github.com/xarray-contrib/xarray.dev/pull/625</a> is merged.</p>

    </main>
  


<script>
            (function (selector, src, preferNativeLazyLoad) {
  var images = document.querySelectorAll(selector);
  var numImages = images.length;

  if (numImages > 0) {
    if (preferNativeLazyLoad && 'loading' in HTMLImageElement.prototype) {
      for (var i = 0; i < numImages; i++) {
        var keys = ['src', 'srcset'];

        for (var j = 0; j < keys.length; j++) {
          if (images[i].hasAttribute('data-' + keys[j])) {
            var value = images[i].getAttribute('data-' + keys[j]);
            images[i].setAttribute(keys[j], value);
          }
        }
      }

      return;
    }

    var script = document.createElement('script');
    script.async = true;
    script.src = src;
    document.body.appendChild(script);
  }
})(
              'img',
              'https://cdn.jsdelivr.net/npm/lazysizes@5/lazysizes.min.js',
              false
            );
          </script></body></html>